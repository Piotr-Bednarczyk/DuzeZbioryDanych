{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c1c730f-21bc-4872-8e42-f4edbc181fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/16 22:28:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, regexp_replace\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "# Inicjalizacja sesji Spark\n",
    "spark = SparkSession.builder.master(\"local[2]\").appName(\"Zadanie_2\").getOrCreate()\n",
    "\n",
    "# Wczytanie pliku zamowienia.txt\n",
    "file_path = \"zamowienia.txt\"\n",
    "df_orders = spark.read.option(\"header\", \"true\").option(\"delimiter\", \";\").csv(file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed32b75b-f527-45bc-a432-5031bed91935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Kraj: string (nullable = true)\n",
      " |-- Sprzedawca: string (nullable = true)\n",
      " |-- Data zamowienia: string (nullable = true)\n",
      " |-- idZamowienia: string (nullable = true)\n",
      " |-- Utarg: float (nullable = true)\n",
      "\n",
      "+------+----------+---------------+------------+------+\n",
      "|  Kraj|Sprzedawca|Data zamowienia|idZamowienia| Utarg|\n",
      "+------+----------+---------------+------------+------+\n",
      "|Polska|  Kowalski|     16.07.2003|       10248| 440.0|\n",
      "|Polska|  Sowiński|     10.07.2003|       10249|  NULL|\n",
      "|Niemcy|   Peacock|     12.07.2003|       10250|  NULL|\n",
      "|Niemcy| Leverling|     15.07.2003|       10251|654.06|\n",
      "|Niemcy|   Peacock|     11.07.2003|       10252|  NULL|\n",
      "+------+----------+---------------+------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_orders = df_orders.withColumn(\"Sprzedawca\", regexp_replace(col(\"Sprzedawca\"), \"ä\", \"ń\"))\n",
    "df_orders = df_orders.withColumn(\"Utarg\", regexp_replace(col(\"Utarg\"), \",\", \".\"))\n",
    "df_orders = df_orders.withColumn(\"Utarg\", regexp_replace(col(\"Utarg\"), \" z\", \"\").cast(FloatType()))\n",
    "df_orders.printSchema()\n",
    "df_orders.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ead0604f-5e41-422d-b74c-f72473f8a412",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/16 22:28:57 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/opt/spark/work-dir/df_orders.csv.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData Bucketing and Partitioning\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Wczytanie danych\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m df_orders \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mheader\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./df_orders.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Zadanie 2.1 - Wiaderkowanie danych\u001b[39;00m\n\u001b[1;32m     14\u001b[0m bucketed_table_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./bucketed_orders\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/readwriter.py:740\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator):\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/opt/spark/work-dir/df_orders.csv."
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, regexp_replace\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Data Bucketing and Partitioning\").getOrCreate()\n",
    "\n",
    "df_orders = spark.read.option(\"header\", \"true\").csv(\"./df_orders.csv\")\n",
    "\n",
    "# Zadanie 2.1 - Wiaderkowanie danych\n",
    "bucketed_table_path = \"./bucketed_orders\"\n",
    "\n",
    "shutil.rmtree(bucketed_table_path, ignore_errors=True)\n",
    "spark.sql(\"DROP TABLE IF EXISTS orders_bucketed\")\n",
    "\n",
    "df_orders.write.bucketBy(16, \"idZamowienia\").mode(\"overwrite\").saveAsTable(\"orders_bucketed\")\n",
    "\n",
    "avg_utarg_bucketed = spark.sql(\"SELECT AVG(Utarg) FROM orders_bucketed\").collect()[0][0]\n",
    "print(f\"Średni utarg (bucketed): {avg_utarg_bucketed}\")\n",
    "\n",
    "# Zadanie 2.2 - Partycjonowanie danych\n",
    "partitioned_table_path = \"./partitioned_orders\"\n",
    "\n",
    "shutil.rmtree(partitioned_table_path, ignore_errors=True)\n",
    "df_orders.write.partitionBy(\"Kraj\").mode(\"overwrite\").csv(partitioned_table_path)\n",
    "\n",
    "# Zadanie 2.3 - \n",
    "start_time = time.time()\n",
    "avg_utarg_original = spark.sql(\"SELECT AVG(Utarg) FROM orders_bucketed WHERE Kraj = 'Polska'\").collect()[0][0]\n",
    "original_time = time.time() - start_time\n",
    "print(f\"Średni utarg (oryginalne dane): {avg_utarg_original}\")\n",
    "print(f\"Czas wykonania dla danych oryginalnych: {original_time:.2f} sekundy\")\n",
    "\n",
    "partitioned_df = spark.read.option(\"header\", \"true\").csv(partitioned_table_path)\n",
    "\n",
    "partitioned_df.printSchema()\n",
    "\n",
    "partitioned_df = partitioned_df.withColumn(\n",
    "    \"_c5\",\n",
    "    regexp_replace(col(\"_c5\"), \",\", \".\").cast(FloatType()).alias(\"Utarg\")\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "avg_utarg_partitioned = (\n",
    "    partitioned_df.filter(col(\"Kraj\") == \"Polska\")\n",
    "    .groupBy(\"Kraj\")\n",
    "    .avg(\"_c5\")\n",
    "    .collect()[0][1]\n",
    ")\n",
    "partitioned_time = time.time() - start_time\n",
    "\n",
    "print(f\"Średni utarg (partycjonowane dane): {avg_utarg_partitioned}\")\n",
    "print(f\"Czas wykonania dla danych partycjonowanych: {partitioned_time:.2f} sekundy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca68259b-eb15-4642-bab2-3cb699f62bed",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'orders_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sample_1 \u001b[38;5;241m=\u001b[39m \u001b[43morders_df\u001b[49m\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;241m0.1\u001b[39m)\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmonth\u001b[39m\u001b[38;5;124m\"\u001b[39m, col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData zamowienia\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msubstr(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m      2\u001b[0m sample_2 \u001b[38;5;241m=\u001b[39m orders_df\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;241m0.1\u001b[39m)\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnetto\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mround\u001b[39m(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUtarg\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1.23\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m      3\u001b[0m sample_3 \u001b[38;5;241m=\u001b[39m orders_df\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;241m0.1\u001b[39m)\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSprzedawca\u001b[39m\u001b[38;5;124m\"\u001b[39m, col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSprzedawca\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mupper())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'orders_df' is not defined"
     ]
    }
   ],
   "source": [
    "sample_1 = orders_df.sample(0.1).withColumn(\"month\", col(\"Data zamowienia\").substr(4, 2))\n",
    "sample_2 = orders_df.sample(0.1).withColumn(\"netto\", round(col(\"Utarg\") / 1.23, 2))\n",
    "sample_3 = orders_df.sample(0.1).withColumn(\"Sprzedawca\", col(\"Sprzedawca\").upper())\n",
    "sample_4 = orders_df.sample(0.1).withColumn(\"waluta\", lit(\"PLN\"))\n",
    "\n",
    "results = {\n",
    "    \"Sredni_Utarg_Wiaderkowane\": bucketed_avg,\n",
    "    \"Czas_Wiaderkowanie\": bucketed_time,\n",
    "    \"Sredni_Utarg_Original\": avg_utarg_original,\n",
    "    \"Czas_Original\": original_time,\n",
    "    \"Sredni_Utarg_Partycjonowane\": avg_utarg_partitioned,\n",
    "    \"Czas_Partycjonowane\": partitioned_query_time,\n",
    "    \"Czas_Partycjonowanie\": partition_time\n",
    "}\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ae1498-3126-4bc8-9de0-c1e582cef099",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2587b65c-e188-4db8-99ca-b0b826b8711b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
